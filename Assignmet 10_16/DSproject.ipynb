{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6iGptmibHk4"
      },
      "source": [
        "#WEEKS 1‚Äì8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI_Rah_4ZMeJ"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Top Cell: Import Libraries\n",
        "# ===========================\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Scikit-learn modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Load Dataset\n",
        "# ===========================\n",
        "df = pd.read_csv('telecom_customer_churn_cleaned.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0XlQUX9T73WV",
        "outputId": "d71fa6fc-3a17-421d-eea7-5597c6f6840e"
      },
      "outputs": [],
      "source": [
        "# View summary\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Drop irrelevant columns (example: ID, customer name, etc. ‚Äî adjust as needed)\n",
        "irrelevant_cols = ['Customer ID', 'Name', 'Unnamed: 0']  # change according to your dataset\n",
        "df = df.drop(columns=[col for col in irrelevant_cols if col in df.columns], errors='ignore')\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Handle NaN values\n",
        "# Option 1: Drop rows with too many NaNs\n",
        "df = df.dropna(thresh=len(df.columns) - 2)  # keeps rows with at least n-2 non-NaN values\n",
        "\n",
        "# Option 2: Fill remaining NaNs\n",
        "df = df.fillna(df.median(numeric_only=True))  # numeric columns\n",
        "df = df.fillna(df.mode().iloc[0])  # categorical columns\n",
        "\n",
        "# Verify cleaning\n",
        "print(\"Remaining NaN values per column:\\n\", df.isna().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyYbuMeiB6WW"
      },
      "source": [
        "# Week 5: Supervised Learning ‚Äì Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vmp4DUny8O_z",
        "outputId": "6e645ec1-a8b6-41c1-8140-fef6b88961b2"
      },
      "outputs": [],
      "source": [
        "# Show columns for reference\n",
        "print(\"Columns in dataset:\\n\", df.columns.tolist())\n",
        "\n",
        "# Select numeric columns only\n",
        "df_numeric = df.select_dtypes(include=[np.number]).dropna()\n",
        "\n",
        "# Try to find the correct target column automatically\n",
        "target_candidates = ['Monthly Charges', 'MonthlyCharges', 'Total Charges', 'TotalCharges', 'Monthly_Fee']\n",
        "target_col = None\n",
        "for col in target_candidates:\n",
        "    if col in df_numeric.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if not target_col:\n",
        "    raise KeyError(\"‚ö†Ô∏è Could not find a numeric target column (e.g., Monthly Charges or Total Charges). \"\n",
        "                   \"Please check your dataset column names.\")\n",
        "\n",
        "print(f\"\\n‚úÖ Using '{target_col}' as the target variable.\\n\")\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df_numeric.drop(columns=[target_col])\n",
        "y = df_numeric[target_col]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear Regression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Evaluate with MAE and RMSE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "# Optional: compare actual vs predicted\n",
        "comparison = pd.DataFrame({'Actual': y_test.values, 'Predicted': y_pred})\n",
        "print(\"\\nSample predictions:\\n\", comparison.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjpdZmnbCH8D"
      },
      "source": [
        "# Week 6: Supervised Learning ‚Äì Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JJF5Xmhs9Soh",
        "outputId": "784ff84d-3074-4888-ae78-31833966578e"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant columns (if exist)\n",
        "irrelevant_cols = ['Customer ID', 'Name', 'Unnamed: 0']\n",
        "df = df.drop(columns=[col for col in irrelevant_cols if col in df.columns], errors='ignore')\n",
        "\n",
        "# Identify target column automatically\n",
        "target_candidates = ['Customer Status', 'Churn', 'Exited', 'Target']\n",
        "target_col = None\n",
        "for col in target_candidates:\n",
        "    if col in df.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if not target_col:\n",
        "    raise KeyError(\"‚ö†Ô∏è Could not find churn/target column. Please verify your dataset.\")\n",
        "\n",
        "print(f\"‚úÖ Using '{target_col}' as target column.\\n\")\n",
        "\n",
        "# Encode target variable (e.g., Churned=1, Stayed=0)\n",
        "le = LabelEncoder()\n",
        "df[target_col] = le.fit_transform(df[target_col])\n",
        "\n",
        "# Convert categorical columns into numeric using one-hot encoding\n",
        "X = df.drop(columns=[target_col])\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "y = df[target_col]\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(X.median(numeric_only=True))\n",
        "X = X.fillna(X.mode().iloc[0])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "log_model = LogisticRegression(solver='saga', max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "log_pred = log_model.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracies\n",
        "log_acc = accuracy_score(y_test, log_pred)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {log_acc:.3f}\")\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.3f}\")\n",
        "\n",
        "best_model = \"Random Forest\" if rf_acc > log_acc else \"Logistic Regression\"\n",
        "print(f\"\\nüèÜ Best Model: {best_model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN2DWnAuBMfD"
      },
      "source": [
        "# **Week 7: Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "collapsed": true,
        "id": "EE2OdDXrBfDz",
        "outputId": "918cad15-c330-4e2f-c102-29fb2a56bdf6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use the trained model and test data from Week 6\n",
        "# (rf_model, X_test, y_test must already be defined)\n",
        "\n",
        "# === Classification Report ===\n",
        "print(\"=== Classification Report ===\")\n",
        "y_pred = rf_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "# === Multiclass ROC & AUC ===\n",
        "n_classes = len(np.unique(y_test))\n",
        "\n",
        "# Get prediction probabilities\n",
        "y_prob = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Binarize labels for ROC\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
        "\n",
        "# Compute ROC and AUC for each class\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC Curves\n",
        "colors = cycle(['blue', 'green', 'red', 'orange', 'purple'])\n",
        "plt.figure(figsize=(7, 6))\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
        "\n",
        "# Baseline line\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "\n",
        "# Graph details\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multiclass ROC Curve ‚Äì Random Forest')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# === Reflection ===\n",
        "print(\"\\nüí¨ Reflection:\")\n",
        "print(\"For churn prediction, Recall is the most important metric ‚Äî we want to identify as many customers likely to churn as possible, even if that means a few false positives.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU3ObVrwDUCz"
      },
      "source": [
        "# Week 8: Unsupervised Learning ‚Äì Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "s_X4IsFPDg3X",
        "outputId": "8c4ab5e5-cc1a-4be5-d69e-0e227477241b"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant columns (optional)\n",
        "irrelevant_cols = ['Customer ID', 'Name', 'Unnamed: 0']\n",
        "df = df.drop(columns=[col for col in irrelevant_cols if col in df.columns], errors='ignore')\n",
        "\n",
        "# Select numeric columns\n",
        "df_num = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Fill missing numeric values (so row count stays the same)\n",
        "df_num = df_num.fillna(df_num.median(numeric_only=True))\n",
        "\n",
        "# Standardize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_num)\n",
        "\n",
        "# Apply K-Means (choose 3 clusters)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels back to the dataframe\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Apply PCA for 2D visualization\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_result[:,0], pca_result[:,1], c=clusters, cmap='viridis')\n",
        "plt.title('K-Means Clustering Visualization (PCA 2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Show sample results\n",
        "print(\"‚úÖ Clustering complete!\")\n",
        "print(df[['Cluster']].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 9: Neural Networks ‚Äì ANN Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assignment 9: Build a simple ANN (Artificial Neural Network)\n",
        "# Using Keras to build an ANN on the classification dataset\n",
        "\n",
        "try:\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Dropout\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    \n",
        "    # Use X_train, X_test, y_train, y_test from Week 6 (classification)\n",
        "    ann = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(len(np.unique(y_train)), activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    ann.compile(optimizer=Adam(learning_rate=0.001), \n",
        "                loss='sparse_categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # Train\n",
        "    ann.fit(X_train, y_train, epochs=10, batch_size=32, \n",
        "            validation_split=0.2, verbose=0)\n",
        "    \n",
        "    # Evaluate\n",
        "    ann_loss, ann_acc = ann.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"‚úÖ ANN Accuracy: {ann_acc:.4f}\")\n",
        "    print(f\"Random Forest Accuracy (Week 6): {rf_acc:.4f}\")\n",
        "    print(f\"Comparison: ANN performs {'better' if ann_acc > rf_acc else 'comparably'} to Random Forest\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è TensorFlow not installed. Skipping ANN (optional for course completion).\")\n",
        "    print(\"To install: pip install tensorflow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 10: Advanced Deep Learning ‚Äì CNN/RNN (Justified Exclusion)\n",
        "\n",
        "**Why not CNN/RNN?**\n",
        "- CNNs (Convolutional Neural Networks) are for image data ‚Üí Our dataset is **tabular** (rows/columns), not images.\n",
        "- RNNs (Recurrent Neural Networks) are for sequential/time-series data ‚Üí Our dataset has **no time dependency**.\n",
        "- For tabular data, **dense/fully-connected ANN** (Week 9) is appropriate.\n",
        "\n",
        "**Conclusion**: CNN/RNN not applicable to telecom customer churn dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11: Natural Language Processing (NLP) ‚Äì Justified Exclusion\n",
        "\n",
        "**Why not NLP?**\n",
        "- NLP (Tokenization, embeddings, TF-IDF) is for **text data** (reviews, messages, documents).\n",
        "- Our dataset is **numeric/categorical telecom features** (tenure, charges, contract type), not text.\n",
        "- No text column to process.\n",
        "\n",
        "**Conclusion**: NLP pipeline not applicable to this project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 12: AI in Data Science ‚Äì Industry Application\n",
        "\n",
        "## Real-World Application: Telecom Customer Churn Prediction\n",
        "\n",
        "**Problem**: Telecom companies lose revenue when customers churn (cancel service).\n",
        "\n",
        "**Solution**: Use ML model to predict which customers are likely to churn, enabling:\n",
        "- **Proactive retention campaigns** (discounts, offers for at-risk customers)\n",
        "- **Cost savings**: Retaining 1 customer is cheaper than acquiring a new one\n",
        "- **Data-driven strategy**: Identify top churn drivers (contract type, charges, internet service)\n",
        "\n",
        "**Business Impact**:\n",
        "- Reduce churn rate by 10‚Äì15% ‚Üí millions in retained revenue\n",
        "- Personalize customer experience based on risk score\n",
        "- Optimize marketing budget allocation\n",
        "\n",
        "**Model Evaluation**: Recall is critical here (catch as many churners as possible, even with false positives).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 13: Model Deployment\n",
        "\n",
        "## Deploy Model on Local Server\n",
        "\n",
        "The trained model is saved as `model.pkl`, `scaler.pkl`, and `feature_columns.pkl`.\n",
        "\n",
        "**To deploy:**\n",
        "\n",
        "1. Run training script:\n",
        "   ```bash\n",
        "   python train_model.py\n",
        "   ```\n",
        "\n",
        "2. Start Flask server:\n",
        "   ```bash\n",
        "   python app.py\n",
        "   ```\n",
        "   Server runs on `http://127.0.0.1:5000`\n",
        "\n",
        "3. Make predictions via API:\n",
        "   ```bash\n",
        "   curl -X POST http://127.0.0.1:5000/predict \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -d '{\"features\": {\"tenure\": 24, \"MonthlyCharges\": 65.5}}'\n",
        "   ```\n",
        "\n",
        "**Response**: `{\"prediction\": 0, \"probability\": 0.85, \"class_probabilities\": [0.85, 0.15]}`\n",
        "\n",
        "See `app.py` for code and `README.md` for detailed instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 14: Model Explainability ‚Äì SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assignment 14: Explain predictions using SHAP\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    \n",
        "    # Use Random Forest from Week 6\n",
        "    explainer = shap.TreeExplainer(rf_model)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    \n",
        "    # Summary plot showing feature importance\n",
        "    print(\"‚úÖ SHAP Summary Plot:\")\n",
        "    shap.summary_plot(shap_values[1] if isinstance(shap_values, list) else shap_values, X_test, show=False)\n",
        "    \n",
        "    print(\"\\nHAP Interpretation:\")\n",
        "    print(\"- Shows which features push prediction toward Churn (red) or No Churn (blue)\")\n",
        "    print(\"- Red = increases churn probability\")\n",
        "    print(\"- Blue = decreases churn probability\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"SHAP not installed. Install with: pip install shap\")\n",
        "    print(\"\\nAlternative explanation: Check feature importance from Random Forest\")\n",
        "    print(f\"Top 5 features: {sorted(zip(X_test.columns, rf_model.feature_importances_), key=lambda x: x[1], reverse=True)[:5]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
